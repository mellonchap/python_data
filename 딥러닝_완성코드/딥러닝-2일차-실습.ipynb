{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada9d9a5",
   "metadata": {},
   "source": [
    "# 딥러닝\n",
    "\n",
    "## 4. 딥러닝 활용 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058e2eb",
   "metadata": {},
   "source": [
    "### [실습] Tensorflow-Keras를 이용한 당뇨 예측 신경망 모델 만들기\n",
    "\n",
    "- **신경망 모델 훈련 단계**\n",
    "1. 학습 데이터 준비\n",
    "2. 신경망 모델 정의\n",
    "3. 컴파일 과정 설정\n",
    "4. 모델 훈련\n",
    "5. 예측 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332458f",
   "metadata": {},
   "source": [
    "#### 1.학습 데이터 준비\n",
    "피마 인디언 당뇨병 데이터(diabetes.csv)\n",
    "- https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
    "- 21세 이상의 피마 인디언 혈통 여성 데이터\n",
    "(all patients here are females at least 21 years old of Pima Indian heritage)\n",
    "> - Pregnancies: 과거 임신 횟수\n",
    "> - Glucose: 혈당 수치\n",
    "> - BloodPressure: 혈압\n",
    "> - SkinThickness: 피부겹두께\n",
    "> - Insulin: 인슐린 농도\n",
    "> - BMI: 체질량 지수(Body Mass Index) \n",
    "> - DiabetesPedigreeFunction: 가족력을 바탕으로 환자가 당뇨에 얼마나 취약한지를 요약한 점수\n",
    "> - Age: 나이\n",
    "> - **Outcome**: (레이블) 예측 목표 변수(최초 측정 후 5년 내 당뇨 발병=1, 미발병=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a014e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47563e80",
   "metadata": {},
   "source": [
    "##### 컬럼 히스토그램으로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eaf51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.hist()\n",
    "plt.tight_layout() # 서브 플롯의 크기/간격 일정하게\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ada7d",
   "metadata": {},
   "source": [
    "##### 결측치 처리\n",
    "- 0인 데이터 --> NaN--> 평균값으로 대체\n",
    "- replace(0, np.nan) --> fillna(df[컬럼].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3015c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe860a",
   "metadata": {},
   "source": [
    "- **1단계**: 0인 데이터 --> NaN : replace(0, np.nan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "for col in cols:\n",
    "    df[col] = df[col].replace(0, np.nan)\n",
    "    \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d4656",
   "metadata": {},
   "source": [
    "- **2단계**: NaN 데이터 --> fillna(df[컬럼].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a9337",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b379fa",
   "metadata": {},
   "source": [
    "##### 데이터 표준화\n",
    "- 범위가 다른 각 컬럼의 수치 데이터들을 평균 0, 분산 1이 되도록 변환\n",
    "- 사이킷런 preprocessing 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150e0b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df_scaled = preprocessing.scale(df)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "df_scaled\n",
    "df_scaled['Outcome'] = df['Outcome']\n",
    "df = df_scaled\n",
    "df\n",
    "\n",
    "df.describe().loc[['min','mean','max','std']].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e93e2d",
   "metadata": {},
   "source": [
    "##### 데이터셋 분할\n",
    "- 사이킷런 train_test_split 이용해서 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:, df.columns!='Outcome']\n",
    "Y = df.loc[:, 'Outcome']\n",
    "\n",
    "x_train, x_test, y_train, y_test =  train_test_split(X, Y, test_size=0.2)\n",
    "print(f'x_train, x_test : {x_train.shape, x_test.shape}')\n",
    "print(f'y_train, y_test : {y_train.shape, y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5a16e",
   "metadata": {},
   "source": [
    "#### 2.신경망 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(Dense(32, activation='relu', input_dim=8)) # 첫 번째 은닉층\n",
    "model.add(Dense(16, activation='relu')) # 두 번째 은닉층\n",
    "model.add(Dense(1, activation='sigmoid')) # 출력 레이어\n",
    "\n",
    "\n",
    "# 방법2\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_dim=8),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')          \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfe21a",
   "metadata": {},
   "source": [
    "#### 3.컴파일 과정 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2143d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dddccf5",
   "metadata": {},
   "source": [
    "#### 4.모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c45a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e83c5c",
   "metadata": {},
   "source": [
    "#### 5.예측 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train)\n",
    "print(f'Training Accuracy: {accuracy*100:>.2f}%')\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Testing Accuracy: {accuracy*100:>.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측해보기\n",
    "#8\t183\t64\t0\t0\t23.3\t0.672\t32\t1\n",
    "#1\t89\t66\t23\t94\t28.1\t0.167\t21\t0\n",
    "# test = np.array([[8,183,64,0,0,23.3,0.672,32]]) \n",
    "# test = np.array([[1,89,66,23,94,28.1,0.167,21]]) \n",
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b291af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63388e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca883122",
   "metadata": {},
   "source": [
    "### [실습] 보스턴 집값 예측모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ce271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# 1. 학습 데이터 준비\n",
    "#---------------------------\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/HousingData.csv')\n",
    "df.head()\n",
    "\n",
    "# 컬럼 히스토그램으로 시각화\n",
    "# import matplotlib.pyplot as plt\n",
    "# df.hist()\n",
    "# plt.tight_layout() # 서브 플롯의 크기/간격 일정하게\n",
    "# plt.show()\n",
    "\n",
    "# 데이터 표준화\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_scaled = preprocessing.scale(df)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "df_scaled\n",
    "df_scaled['MEDV'] = df['MEDV']\n",
    "df = df_scaled\n",
    "df\n",
    "\n",
    "df.describe().loc[['min','mean','max','std']].round(2)\n",
    "\n",
    "\n",
    "# 데이터셋 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:, df.columns!='MEDV']\n",
    "Y = df.loc[:, 'MEDV']\n",
    "\n",
    "x_train, x_test, y_train, y_test =  train_test_split(X, Y, test_size=0.2)\n",
    "print(f'x_train, x_test : {x_train.shape, x_test.shape}')\n",
    "print(f'y_train, y_test : {y_train.shape, y_test.shape}')\n",
    "x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 학습 데이터 준비\n",
    "from tensorflow.keras.datasets import boston_housing   \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "print(type(x_train))\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "\n",
    "# 데이터 정규화\n",
    "# 독립변수(Feature에 대한 데이터 정규화 진행)\n",
    "# 특성의 중앙이 0 근처에 맞추어지고 표준 편차를 1이 되도록한다.\n",
    "x_mean = x_train.mean(axis=0)\n",
    "x_std = x_train.std(axis=0)\n",
    "x_train -= x_mean\n",
    "x_train /= x_std\n",
    "x_test -= x_mean\n",
    "x_test /= x_std\n",
    "x_train\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(x_train)      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7197eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# 2. 신경망 모델 정의\n",
    "#---------------------------\n",
    "# 방법1\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(Dense(64, activation='relu', input_dim=13)) # 첫 번째 은닉층\n",
    "model.add(Dense(64, activation='relu')) # 두 번째 은닉층\n",
    "model.add(Dense(1)) # 출력 레이어\n",
    "\n",
    "#---------------------------\n",
    "# 3. 컴파일 과정 설정\n",
    "#---------------------------\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='mse',\n",
    "             metrics=['mae'])\n",
    "\n",
    "#---------------------------\n",
    "# 4. 모델 훈련\n",
    "#---------------------------\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "#---------------------------\n",
    "# 5. 예측 및 평가\n",
    "#---------------------------\n",
    "mse_score, mae_score = model.evaluate(x_test, y_test)\n",
    "print(f'Testing mse_score: {mse_score:>.2f}%')\n",
    "print(f'Testing mae_score: {mae_score:>.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227cada",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac0123",
   "metadata": {},
   "source": [
    "### [실습] Teachable Machine(Tensorflow) 모델 이용하여 마스크 착용 판별여부 판단하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a65b4",
   "metadata": {},
   "source": [
    "- 모델생성 : Teachable Machine(Tensorflow)\n",
    "- **구현할 부분**:  실시간으로 카메라로 마스크 착용여부 판단하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e640af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import win32com.client\n",
    "\n",
    "# 케라스 모델 가져오기\n",
    "model = tf.keras.models.load_model('converted_keras/keras_model.h5')\n",
    "\n",
    "\n",
    "#-------------------\n",
    "# 실시간 카메라 작동\n",
    "#-------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print('Video open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "CONFIDENCE = 0.8\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Video read failed!')\n",
    "        break\n",
    "    \n",
    "    #-------------------\n",
    "    # 마스크 착용 판별하기 ---\n",
    "    # 모델에서 원하는 사이즈로 resizing\n",
    "    size = (224, 224)\n",
    "    frame_resized = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 이미지 정규화            \n",
    "    frame_normalized = (frame_resized.astype(np.float32) / 127.0) - 1\n",
    "    \n",
    "    # keras에서 원하는 모양(차원으로 변환)\n",
    "    frame_reshape = frame_normalized.reshape(1, 224, 224, 3)\n",
    "    \n",
    "    # 예측결과\n",
    "    prediction = model.predict(frame_reshape)\n",
    "#     print(prediction.shape)\n",
    "#     print(prediction)\n",
    "    \n",
    "    # prediction: [[0.13399076 0.86600924]]\n",
    "    if prediction[0, 0] > CONFIDENCE : # 0(mask)일때 정확도\n",
    "        cv2.putText(frame, 'Mask', (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,0), 2, cv2.LINE_AA) # 검정\n",
    "#         print('mask')\n",
    "    elif prediction[0, 1] > CONFIDENCE : # 2(nmask)일때 정확도\n",
    "        cv2.putText(frame, 'Nomask', (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2, cv2.LINE_AA) # 빨강\n",
    "#         print('nomask')\n",
    "    \n",
    "    #-------------------\n",
    "            \n",
    "    cv2.imshow('Mask Detect', frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key==27: # ESC키 눌러졌을 때 프로그램 나가기\n",
    "        break\n",
    "    elif key==13:\n",
    "        cv2.imwrite('image/photo.jpg', frame)\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('Video Finish!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f23db3",
   "metadata": {},
   "source": [
    "#### TTS 기능 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a384087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윈도우에서 tts사용 라이브러리 설치\n",
    "!pip install pywin32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import win32com.client\n",
    "\n",
    "# win TTS 사용\n",
    "tts = win32com.client.Dispatch(\"SAPI.SpVoice\")\n",
    "\n",
    "# 케라스 모델 가져오기\n",
    "model = tf.keras.models.load_model('converted_keras/keras_model.h5')\n",
    "\n",
    "\n",
    "#-------------------\n",
    "# 실시간 카메라 작동\n",
    "#-------------------\n",
    "tts.Speak(\"카메라가 실행됩니다.\")  \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print('Video open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "CONFIDENCE = 0.8\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Video read failed!')\n",
    "        break\n",
    "    \n",
    "    #-------------------\n",
    "    # 마스크 착용 판별하기 ---\n",
    "    # 모델에서 원하는 사이즈로 resizing\n",
    "    size = (224, 224)\n",
    "    frame_resized = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 이미지 정규화            \n",
    "    frame_normalized = (frame_resized.astype(np.float32) / 127.0) - 1\n",
    "    \n",
    "    # keras에서 원하는 모양(차원으로 변환)\n",
    "    frame_reshape = frame_normalized.reshape(1, 224, 224, 3)\n",
    "    \n",
    "    # 예측결과\n",
    "    prediction = model.predict(frame_reshape)\n",
    "#     print(prediction.shape)\n",
    "#     print(prediction)\n",
    "    \n",
    "    # prediction: [[0.13399076 0.86600924]]\n",
    "    if prediction[0, 0] > CONFIDENCE : # 0(mask)일때 정확도\n",
    "        cv2.putText(frame, 'Mask', (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,0), 2, cv2.LINE_AA) # 검정\n",
    "#         print('mask')\n",
    "    elif prediction[0, 1] > CONFIDENCE : # 2(nmask)일때 정확도\n",
    "        cv2.putText(frame, 'Nomask', (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2, cv2.LINE_AA) # 빨강\n",
    "#         print('nomask')\n",
    "        tts.Speak('마스크를 착용하세요!')\n",
    "    \n",
    "    #-------------------\n",
    "            \n",
    "    cv2.imshow('Mask Detect', frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key==27: # ESC키 눌러졌을 때 프로그램 나가기\n",
    "        break\n",
    "\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('Video Finish!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990a116",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f9240",
   "metadata": {},
   "source": [
    "### [실습] 마우스로 직접 쓴 숫자 인식하는 프로그램 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a77e3d",
   "metadata": {},
   "source": [
    "#### # Tensorflow MNIST 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "oldx, oldy = -1, -1\n",
    "\n",
    "# -------------------\n",
    "# 마우스 이벤트\n",
    "# -------------------\n",
    "def on_mouse(event, x, y, flags, _):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        oldx, oldy = -1, -1\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), (255, 255, 255), 40, cv2.LINE_AA)\n",
    "            oldx, oldy = x, y\n",
    "            cv2.imshow('img', img)\n",
    "                        \n",
    "# -------------------\n",
    "# 1. 학습모델 만들기\n",
    "# -------------------\n",
    "# Tensorflow MNIST 모델 만들기\n",
    "# 1.학습 데이터 준비\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# MNIST 데이터 로딩하기\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "# 2.신경망 모델 정의\n",
    "# 방법1\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# 3.컴파일 과정 설정\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4.모델 훈련\n",
    "model.fit(x_train, y_train, epochs=5, verbose=1)\n",
    "\n",
    "# 5.예측 및 평가\n",
    "model.evaluate(x_test, y_test)\n",
    "           \n",
    "    \n",
    "    \n",
    "# -------------------\n",
    "# 2. 입력 창 만들어 숫자 인식하기\n",
    "# -------------------\n",
    "# 사용자 입력 영상에 대해 예측\n",
    "img = np.zeros((400, 400), np.uint8)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.setMouseCallback('img', on_mouse)\n",
    "\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey()\n",
    "    if key == 27:   # ESC 키\n",
    "        break\n",
    "    elif key == 13 : # 엔터 키\n",
    "        # Tensorflow 모델에 넣어서 예측하기\n",
    "        test_image = cv2.resize(img, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "        test_image = test_image.reshape(1, 28, 28)   # np.expand_dims(test_image, axis=0)\n",
    "        prediction = model.predict(test_image)  \n",
    "        print(f'prediction : {prediction}')\n",
    "        print(f'Tensorflow 모델로 인식된 숫자는: {np.argmax(prediction[0])}')\n",
    "        \n",
    "        # 화면 clear기능이 되도록 구현\n",
    "        img.fill(0) # 검은색으로 채우기 -->clear기능이 되도록 구현\n",
    "        cv2.imshow('img', img)\n",
    "        \n",
    "cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648355c9",
   "metadata": {},
   "source": [
    "#### OpenCV KNN MNIST 숫자인식 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "oldx, oldy = -1, -1\n",
    "\n",
    "# -------------------\n",
    "# 마우스 이벤트\n",
    "# -------------------\n",
    "def on_mouse(event, x, y, flags, _):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        oldx, oldy = -1, -1\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), (255, 255, 255), 40, cv2.LINE_AA)\n",
    "            oldx, oldy = x, y\n",
    "            cv2.imshow('img', img)\n",
    "                        \n",
    "# -------------------\n",
    "# 1. 학습모델 만들기\n",
    "# -------------------\n",
    "# OpenCV KNN MNIST 모델 만들기\n",
    "# 이미지를 직접 쪼개서 학습용 데이터와 레이블을 만들고 KNN 알고리즘에 넣어 ML 모델 만들기\n",
    "digits = cv2.imread('image/digits.png', cv2.IMREAD_GRAYSCALE)\n",
    "h, w = digits.shape\n",
    "print(f'digits.shape: {digits.shape}')\n",
    "print(f'h, w: {h, w}')\n",
    "\n",
    "# 학습용 이미지크기로 조정하기(20x20)\n",
    "cells = [np.hsplit(row, w//20)   for row in np.vsplit(digits, h//20)]\n",
    "cells = np.array(cells)\n",
    "print(f'cells.shape : {cells.shape}')\n",
    "\n",
    "# 학습용 데이터 & 레이블 만들기(지도학습 데이터 만들기)\n",
    "train_images = cells.reshape(-1, 400).astype(np.float32)\n",
    "train_labels = np.repeat(np.arange(10), len(train_images)/10)\n",
    "print(f'train_images : {train_images.shape}') # 0~9이미지가 각각 500개씩\n",
    "print(f'train_labels : {train_labels}') # 0~9이미지의 레이블값\n",
    "\n",
    "# OpenCV KNN 알고리즘 넣어 학습시키기\n",
    "knn_model = cv2.ml.KNearest_create()\n",
    "knn_model.train(train_images, cv2.ml.ROW_SAMPLE, train_labels )\n",
    "    \n",
    "    \n",
    "# -------------------\n",
    "# 2. 입력 창 만들어 숫자 인식하기\n",
    "# -------------------\n",
    "# 사용자 입력 영상에 대해 예측\n",
    "img = np.zeros((400, 400), np.uint8)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.setMouseCallback('img', on_mouse)\n",
    "\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey()\n",
    "    if key == 27:   # ESC 키\n",
    "        break\n",
    "    elif key == 13 : # 엔터 키\n",
    "        # OpenCV KNN모델에 넣어서 예측하기\n",
    "        test_image = cv2.resize(img, (20, 20), interpolation=cv2.INTER_AREA)\n",
    "        test_image = test_image.reshape(-1, 400).astype(np.float32)\n",
    "        prediction = knn_model.findNearest(test_image, 5)   # k=5\n",
    "#         print(f'prediction : {prediction}')\n",
    "        print(f'OpenCV KNN 모델로 인식된 숫자는: {int(prediction[0])}')\n",
    "        \n",
    "        # 화면 clear기능이 되도록 구현\n",
    "        img.fill(0) # 검은색으로 채우기 -->clear기능이 되도록 구현\n",
    "        cv2.imshow('img', img)\n",
    "        \n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797f6cc",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc33eb",
   "metadata": {},
   "source": [
    "### [실습] OpenCV DNN을 이용한 얼굴 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03187708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Finish!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print('Camera open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 얼굴인식용 DNN Caffe모델 가져오기\n",
    "# model = 'face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "# config= 'face_detector/deploy.prototxt'\n",
    "model  = 'face_detector/opencv_face_detector_uint8.pb'\n",
    "config = 'face_detector/opencv_face_detector.pbtxt'\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "if net.empty():\n",
    "    print('readNet failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 실시간으로 얼굴 인식하기    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Camera read failed!')\n",
    "        break\n",
    "    \n",
    "    # DNN에 맞는 얼굴인식용 이미지로 변환하기\n",
    "    frame_blob = cv2.dnn.blobFromImage(frame, 1, (300,300), (104, 177, 123))\n",
    "    net.setInput(frame_blob)    \n",
    "    detect = net.forward()\n",
    "    detect = detect[0, 0, :, :]\n",
    "        \n",
    "    # 인식된 얼굴에 사각형 그리기\n",
    "    h, w, _ = frame.shape\n",
    "    for i in range(detect.shape[0]):\n",
    "#         print(detect)\n",
    "        \n",
    "        confidence = detect[i, 2]\n",
    "        if confidence < 0.5:\n",
    "            break\n",
    "        \n",
    "        # 얼굴의 위치(x,y)좌표\n",
    "        x1 = int(detect[i,3] * w)\n",
    "        y1 = int(detect[i,4] * h)\n",
    "        x2 = int(detect[i,5] * w)\n",
    "        y2 = int(detect[i,6] * h)\n",
    "        \n",
    "        # 얼굴에 박스 그리기\n",
    "        cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "        \n",
    "        # 정확도 숫자 글씨\n",
    "        title = f'Face: {confidence:4.2f}'\n",
    "        cv2.putText(frame, title, (x1, y1-1), cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                    (0,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key==27: # ESC키 눌러졌을 때 프로그램 나가기\n",
    "        break\n",
    "\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('Camera Finish!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4a252",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf8f76",
   "metadata": {},
   "source": [
    "### [실습] YOLO를 이용한 사물인식(이미지 파일 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c461691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41]\n",
      " [46]\n",
      " [47]] cup\n",
      "정확도:\n",
      "[[0.9972165]\n",
      " [0.9082982]\n",
      " [0.8414951]]\n",
      "박스위치:n[[667  33 200 586]\n",
      " [ 94 357 218 283]\n",
      " [372 443 176 178]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# img = cv2.imread('image/dog.jpg')\n",
    "img = cv2.imread('image/fruits.jpg')\n",
    "\n",
    "# 모델 & 설정 파일\n",
    "model  = 'yolo_v4/yolov4.weights'\n",
    "config = 'yolo_v4/yolov4.cfg'\n",
    "labels = 'yolo_v4/coco.names' # 'yolo_v4/voc.names'\n",
    "\n",
    "\n",
    "# 1.객체 라벨 가져오기\n",
    "with open(labels, 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "# print(f'라벨데이터: {len(classes)}, {classes}')\n",
    "    \n",
    "# 2.OpenCV DNN 클래스 사용    \n",
    "net = cv2.dnn.readNetFromDarknet(config, model) \n",
    "model = cv2.dnn_DetectionModel(net)\n",
    "model.setInputParams(scale=1/255, size=(416, 416), swapRB=True)\n",
    "\n",
    "\n",
    "# 3.객체 검출하면 분류ID, 정확도, 박스 값 리턴하기 \n",
    "# opencv 4.5.4에서 오류\n",
    "# detection 으로 인정할 최소정확도 지정(confThreshold=0.6, nmsThreshold=0.4)\n",
    "classIds, scores, boxes = model.detect(img, confThreshold=0.6, nmsThreshold=0.4)\n",
    "print(classIds, classes[classIds[0][0]] )\n",
    "print(f'정확도:\\n{scores}')\n",
    "print(f'박스위치:n{boxes}')\n",
    "\n",
    "\n",
    "# 4.검출된 객체에 박스, 객체명, 정확도 그리기\n",
    "for (classId, score, box) in zip(classIds, scores, boxes):\n",
    "    cv2.rectangle(img, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]),\n",
    "                  color=(0, 255, 0), thickness=2)\n",
    " \n",
    "    text = '%s: %.2f' % (classes[classId[0]], score)\n",
    "    cv2.putText(img, text, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                color=(0, 255, 0), thickness=2)\n",
    "\n",
    "# 5 화면에 출력하기\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395051cc",
   "metadata": {},
   "source": [
    "### [실습] YOLO를 이용한 실시간 사물인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85dbdd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# -----------------------\n",
    "# 1.YOLO 사용 준비\n",
    "# -----------------------\n",
    "# 모델 & 설정 파일\n",
    "config = 'yolo_v4/yolov4.cfg'\n",
    "model  = 'yolo_v4/yolov4.weights'\n",
    "labels = 'yolo_v4/coco.names'\n",
    "\n",
    "# 객체 라벨 가져오기\n",
    "with open(labels, 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "    \n",
    "# penCV DNN 클래스 사용    \n",
    "net = cv2.dnn.readNetFromDarknet(config, model) \n",
    "model = cv2.dnn_DetectionModel(net)\n",
    "model.setInputParams(scale=1/255, size=(416, 416), swapRB=True)\n",
    "\n",
    "# -----------------------\n",
    "# 2.카메라 사용 준비\n",
    "# -----------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened:\n",
    "    print('--camera open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# -----------------------\n",
    "# 4.YOLO를 이용하여 frame에서 객체 검출\n",
    "# -----------------------\n",
    "# YOLO Object검출 및 출력\n",
    "def detectAndDisplay(frame):\n",
    "    \n",
    "    # 3.객체 검출하면 분류ID, 정확도, 박스 값 리턴하기 \n",
    "    # detection 으로 인정할 최소정확도 지정(confThreshold=0.6, nmsThreshold=0.4)\n",
    "    classIds, scores, boxes = model.detect(frame, confThreshold=0.6, nmsThreshold=0.4)\n",
    "\n",
    "    \n",
    "    # 4.검출된 객체에 박스, 객체명, 정확도 그리기\n",
    "    for (classId, score, box) in zip(classIds, scores, boxes):\n",
    "        cv2.rectangle(frame, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]),\n",
    "                      color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        text = '%s: %.2f' % (classes[classId[0]], score) # classes[classId](오픈CV:4.5.4이상)\n",
    "        cv2.putText(frame, text, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                    color=(0, 255, 0), thickness=2)\n",
    "    return frame\n",
    "\n",
    "# -----------------------\n",
    "# 3.실시간 카메라 frame 불러오기\n",
    "# -----------------------\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret: \n",
    "        print('--Can not read frame---Break!')\n",
    "        break\n",
    "\n",
    "    detectAndDisplay(frame)\n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()       \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d40bb",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c47f13",
   "metadata": {},
   "source": [
    "### [실습] Mediapipe를 얼굴인식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7278b41",
   "metadata": {},
   "source": [
    "#### Mediapipe 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e99c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d07c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "\n",
    "        # Draw the face detection annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                mp_drawing.draw_detection(image, detection)\n",
    "                #print(detection)\n",
    "                \n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ca9b7",
   "metadata": {},
   "source": [
    "### [실습] Mediapipe를 이용한 Hand Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61568d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "#    model_complexity=0,    # 오류가 발생하여 주석표시\n",
    "    max_num_hands=2,  # 인식할 손의 갯수\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c9a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
